{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instacart Reorder Prediction Summary\n",
    "\n",
    "The main objective of this project is to create a model that predicts the products an Instacart customer will buy again in his next order. The intent of this model is not to make predictions on items users have never bought before so, it is not a recommendation engine. It is a classification model that uses data on previous orders to predict what will be in the cart in the next order as a reordered product.\n",
    "\n",
    "The notebooks should be checked in the following order:\n",
    "1. Final_Instacart_Data_Analysis\n",
    "2. Final_Instacart_userid_n\n",
    "3. Final_Instacart_all_users\n",
    "\n",
    "First step of this project is Exploratory Data Analysis which is enclosed by file named \"Final_Instacart_Data_Analysis\". It contains the whole process of loading, checking, exploring and visualizing the data. The techniques used were combining tables to create wider ones, cleaning the data (missing values and non-needed features), analyzing all tables to identify the ones that could be useful to build the model and plot different information to have a broader view of what the data looks like. The data set is huge. It has information about 200.000 costumers' orders and it was already divided into train, validate and test data.\n",
    "\n",
    "Second step is defining what the objective of the model should be. So in this case, we want a model that accepts one customer id and make predictions for that specific customer. Based on this goal, we have a model that accepts a randomly selected user id and runs all the feature engineering, model and predictions for that user, using logistic regression to predict whether or not a previously bought item will be in the cart next time. For coding details check file named \"Final_Instacart_userid_n\".\n",
    "\n",
    "Third step is creating the same model but averaging the results for n number of customers so it can tell us if the model has a good performance in average for any customer in the data. To achive this, we use the same process built for one unique customer id and create a for loop to iterate through the randomly selected n ids, accumulate the results and returns the accuracy of the model for all the n (500 in the model observed in the file) ids. For more details check file named \"Final_Instacart_all_users\". \n",
    "\n",
    "For all ids tested the model showed superior performance than the baseline with good results based on F1-score (classification matrix). In order to decide on the model we tested both decision trees, ensemble models and logistic regression. The results were pretty similar so I decided to select logistic regression.\n",
    "\n",
    "Next steps include optimizing the model so that we get rid of the for loop to predict multiple users reorders and building a model that based on previous orders can recommend items to the customer (items never bought previously)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
